{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Temporal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prashansanb/Machine-Learning-in-Drought-Index-Forecasting/blob/master/Temporal_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yXLy6BFPx_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRWlsnzPP5k6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "wb = gc.open_by_url('https://docs.google.com/spreadsheets/d/1_2CuHRVRqhEQ4oJnjFV-ZARBd_EgXpnHhXy0uABS1ok/edit?usp=sharing')\n",
        "sheet = wb.worksheet('Sheet1')\n",
        "data = sheet.get_all_values()\n",
        "df = pd.DataFrame(data)\n",
        "df.columns = df.iloc[0]\n",
        "df = df.iloc[1:]\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEWq1-PnP_-P",
        "colab_type": "code",
        "outputId": "b62870d5-def7-4001-fc0b-54eaa5c78c8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import sklearn.metrics as mx\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from math import sqrt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import KFold\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn import preprocessing\n",
        "from keras.layers import Dropout\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.constraints import maxnorm\n",
        "from keras.optimizers import SGD\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD\n",
        "from matplotlib import pyplot\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "#Standardization of data\n",
        "names=df.columns\n",
        "scaler = preprocessing.StandardScaler()\n",
        "scaled_df = scaler.fit_transform(df)\n",
        "scaled_df = pd.DataFrame(scaled_df,columns=names)\n",
        "\n",
        "#Splitting of data into test and training as per year\n",
        "x_train = scaled_df.iloc[:1321, 3:17].values.astype(float)\n",
        "y_train = scaled_df.iloc[:1321, 2]\n",
        "x_test = scaled_df.iloc[1322:,3:17].values.astype(float)\n",
        "y_test = scaled_df.iloc[1322:, 2]\n",
        "y_train  = y_train.astype(float)\n",
        "y_test  = y_test.astype(float)\n",
        "batch_size =len(x_train)\n",
        "#print(x_train,y_train)\n",
        "#print(x_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY_e1EA0Rhq1",
        "colab_type": "text"
      },
      "source": [
        "# Support Vector Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs9p6TghQWrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_grid = {'C': [70,80, 50,100,10], 'gamma': [0.01,1,0.1],'kernel': ['rbf'], 'epsilon': [0.06, 0.08, 0.07]}\n",
        "grid = GridSearchCV(SVR(),param_grid,refit=True,verbose=2, scoring='neg_mean_squared_error')\n",
        "grid.fit(x_train,y_train)\n",
        "print(grid.best_estimator_)\n",
        "pred=grid.predict(x_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AV-LJEJHIHuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(r2_score(y_test, pred, sample_weight=None, multioutput='uniform_average')) #R_squared value\n",
        "print(mean_squared_error(y_test, pred))     #MSE value\n",
        "print(mean_absolute_error(y_test, pred))      #MAE value\n",
        "rmse = sqrt(mean_squared_error(y_test,pred))\n",
        "print(rmse)                                    #RMSE value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR83gJsQ1Mrb",
        "colab_type": "text"
      },
      "source": [
        "Feature Ablation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXvcSFrQeJw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Feature Importance using Feature Ablation for SVM\n",
        "def score_mod(x_train, x_test, y_train, y_test):\n",
        "  clf = SVR(gamma=0.01, kernel='rbf',C=70, cache_size=200, coef0=0.0, degree=3, epsilon=0.07)\n",
        "  clf.fit(x_train, y_train)\n",
        "  y_pred = clf.predict(x_test)\n",
        "  return mx.r2_score(y_test, y_pred)\n",
        "base_score= score_mod(x_train, x_test, y_train, y_test)\n",
        "scores = []\n",
        "for i in range(x_train.shape[1]):\n",
        "  use_column = [ndx != i for ndx in range(x_train.shape[1])]\n",
        "  scores.append(score_mod(x_train[:, use_column],x_test[:, use_column],y_train,y_test))\n",
        "\n",
        "sorted(enumerate([base_score - s for s in scores]),key=lambda ndx_score: ndx_score[1],reverse=True)[:13]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJHYfxl0RkDz",
        "colab_type": "text"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-tMuWPBR3Rj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "n_estimators = [int(x) for x in np.linspace(start = 500, stop = 2000, num = 10)]\n",
        "max_features = ['auto', 'sqrt']\n",
        "max_depth = [int(x) for x in np.linspace(30, 90, num = 11)]\n",
        "max_depth.append(None)\n",
        "min_samples_split = [2, 5, 10]\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid for Random Search\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap,\n",
        "               }\n",
        "rf = RandomForestRegressor()\n",
        "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
        "rf_random.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MEBCvSr1kyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_estimators = [1000]\n",
        "max_features = ['auto']\n",
        "max_depth = [50]\n",
        "max_depth.append(None)\n",
        "min_samples_split = [2]\n",
        "min_samples_leaf = [2]\n",
        "param_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               }\n",
        "\n",
        "#Grid search based on the values funnelled by random search\n",
        "\n",
        "gsc = GridSearchCV(RandomForestRegressor(), param_grid,cv=5,scoring='neg_mean_squared_error', verbose=2,n_jobs=-1)\n",
        "grid_result = gsc.fit(x_train, y_train)\n",
        "best_params = grid_result.best_params_\n",
        "print(best_params)\n",
        "pred=gsc.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTijn5DWICxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(r2_score(y_test, pred, sample_weight=None, multioutput='uniform_average')) #R_squared value\n",
        "print(mean_squared_error(y_test, pred))     #MSE value\n",
        "rmse = sqrt(mean_squared_error(y_test,pred))\n",
        "print(rmse)                                     #RMSE value\n",
        "print(mean_absolute_error(y_test, pred))      #MAE value\n",
        "\n",
        "#Feature importance:\n",
        "rfr= RandomForestRegressor(best_params)\n",
        "rfr.fit(x_train,y_train)\n",
        "importance = rfr.feature_importances_\n",
        "print('Feature: %0d, Score: %.5f' % (i,v))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7F50JQESc3g",
        "colab_type": "text"
      },
      "source": [
        "## ANN - Multi Layer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-mx5ON-gAtu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "tuned_parameters = [{'hidden_layer_sizes': [13,500,150,200],\n",
        "'activation': ['relu','logistic'],\n",
        "'solver':['adam','sgd'], 'alpha':[0.0001,0.01], \n",
        "'batch_size':['auto'], 'learning_rate':['invscaling'],\n",
        "'learning_rate_init':[0.001], 'max_iter':[1000]}]\n",
        "rgr = GridSearchCV(MLPRegressor(), tuned_parameters, cv=5, scoring='neg_root_mean_squared_error')\n",
        "grid_result=rgr.fit(x_train, y_train)\n",
        "best_params = grid_result.best_params_\n",
        "print(best_params)\n",
        "pred=rgr.predict(x_test)\n",
        "pred=rgr.predict(x_test)\n",
        "print(r2_score(y_test, pred, sample_weight=None, multioutput='uniform_average')) #R_squared value\n",
        "print(mean_squared_error(y_test, pred))     #MSE value\n",
        "rmse = sqrt(mean_squared_error(y_test,pred))\n",
        "score_feature=rmse\n",
        "print(rmse)                                     #RMSE value\n",
        "print(mean_absolute_error(y_test, pred))      #MAE value\n",
        "#Feature Importance using ablation for MLP\n",
        "def score_mod(x_train, x_test, y_train, y_test):\n",
        "  rgr.fit(x_train, y_train)\n",
        "  y_pred = rgr.predict(x_test)\n",
        "  return mx.r2_score(y_test, y_pred)\n",
        "base_score= score_mod(x_train, x_test, y_train, y_test)\n",
        "scores = []\n",
        "for i in range(x_train.shape[1]):\n",
        "  use_column = [ndx != i for ndx in range(x_train.shape[1])]\n",
        "  scores.append(score_mod(x_train[:, use_column],x_test[:, use_column],y_train,y_test))\n",
        "print(\"R2 score\",base_score)\n",
        "sorted(enumerate([base_score - s for s in scores]),key=lambda ndx_score: ndx_score[1],reverse=True)[:13]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PZVJj51Hud3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#weights for all the nodes in the Multi Layer Perceptron \n",
        "print(weights)\n",
        "print(\"\\n\")\n",
        "pred= mod.predict(x_test)\n",
        "print(np.sqrt(mean_squared_error(y_test,pred)))\n",
        "print(r2_score(y_test, pred, sample_weight=None, multioutput='uniform_average')) #R_squared value"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}